{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Kx5X-m5AK2c9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-DKzGdnoIsAV"
   },
   "outputs": [],
   "source": [
    "\n",
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "Fn2cMtniJigl",
    "outputId": "22b62813-9383-416d-90c8-4d58bd919bb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'a', 'king'], ['she', 'is', 'a', 'queen'], ['he', 'is', 'a', 'man'], ['she', 'is', 'a', 'woman'], ['warsaw', 'is', 'poland', 'capital'], ['berlin', 'is', 'germany', 'capital'], ['paris', 'is', 'france', 'capital']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "S7GDVdmtKGG7"
   },
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "q78TRvzUKQRd",
    "outputId": "6433da10-9703-4dca-9bba-e6c22d644687"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'he': 0,\n",
       " 'is': 1,\n",
       " 'a': 2,\n",
       " 'king': 3,\n",
       " 'she': 4,\n",
       " 'queen': 5,\n",
       " 'man': 6,\n",
       " 'woman': 7,\n",
       " 'warsaw': 8,\n",
       " 'poland': 9,\n",
       " 'capital': 10,\n",
       " 'berlin': 11,\n",
       " 'germany': 12,\n",
       " 'paris': 13,\n",
       " 'france': 14}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m_FnNckvKUXI"
   },
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, threated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "_pRBS-TAKvEA",
    "outputId": "e93a34ce-834f-4e9b-919d-ed0f64f5b5ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 2],\n",
       "       [1, 0],\n",
       "       [1, 2],\n",
       "       [1, 3],\n",
       "       [2, 0],\n",
       "       [2, 1],\n",
       "       [2, 3],\n",
       "       [3, 1],\n",
       "       [3, 2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R76JAnrXLK8s"
   },
   "source": [
    "![alt text](https://miro.medium.com/max/377/1*uYiqfNrUIzkdMrmkBWGMPw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5Xsy5_fAK-qK"
   },
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "  \n",
    "  #Input layer is just the center word encoded in one-hot manner. It dimensions are [1, vocabulary_size]\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aLU--gGwLNLX",
    "outputId": "cd2921e0-6c5e-4d88-abfd-ce6a3ae3777c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo 0: 4.53072595340865\n",
      "Loss at epo 10: 4.0549723642213005\n",
      "Loss at epo 20: 3.7252400296075003\n",
      "Loss at epo 30: 3.4815810373851233\n",
      "Loss at epo 40: 3.292556229659489\n",
      "Loss at epo 50: 3.1397426298686435\n",
      "Loss at epo 60: 3.012433751991817\n",
      "Loss at epo 70: 2.9044359173093524\n",
      "Loss at epo 80: 2.81195342029844\n",
      "Loss at epo 90: 2.7323426961898805\n",
      "Loss at epo 100: 2.663464447430202\n",
      "Loss at epo 110: 2.603437714917319\n",
      "Loss at epo 120: 2.5506201131003245\n",
      "Loss at epo 130: 2.50364739213671\n",
      "Loss at epo 140: 2.4614409719194685\n",
      "Loss at epo 150: 2.4231791981628965\n",
      "Loss at epo 160: 2.3882445079939707\n",
      "Loss at epo 170: 2.356171919618334\n",
      "Loss at epo 180: 2.326606409038816\n",
      "Loss at epo 190: 2.2992689217839923\n",
      "Loss at epo 200: 2.2739326545170377\n",
      "Loss at epo 210: 2.25040830884661\n",
      "Loss at epo 220: 2.2285300740173883\n",
      "Loss at epo 230: 2.2081513336726597\n",
      "Loss at epo 240: 2.1891385316848755\n",
      "Loss at epo 250: 2.1713689046246665\n",
      "Loss at epo 260: 2.154729962348938\n",
      "Loss at epo 270: 2.139118781260082\n",
      "Loss at epo 280: 2.124439911331449\n",
      "Loss at epo 290: 2.1106070833546773\n",
      "Loss at epo 300: 2.0975419385092597\n",
      "Loss at epo 310: 2.085173691170556\n",
      "Loss at epo 320: 2.0734393451895032\n",
      "Loss at epo 330: 2.062282090527671\n",
      "Loss at epo 340: 2.0516515391213552\n",
      "Loss at epo 350: 2.0415028997829983\n",
      "Loss at epo 360: 2.0317963919469286\n",
      "Loss at epo 370: 2.022496513383729\n",
      "Loss at epo 380: 2.0135715586798533\n",
      "Loss at epo 390: 2.0049930197852\n",
      "Loss at epo 400: 1.99673541699137\n",
      "Loss at epo 410: 1.9887761060680662\n",
      "Loss at epo 420: 1.981094056367874\n",
      "Loss at epo 430: 1.9736708096095494\n",
      "Loss at epo 440: 1.966489064693451\n",
      "Loss at epo 450: 1.9595334589481355\n",
      "Loss at epo 460: 1.9527898439339229\n",
      "Loss at epo 470: 1.9462449967861175\n",
      "Loss at epo 480: 1.9398872452122824\n",
      "Loss at epo 490: 1.9337054984910147\n",
      "Loss at epo 500: 1.9276900636298315\n",
      "Loss at epo 510: 1.9218316584825517\n",
      "Loss at epo 520: 1.9161216620888029\n",
      "Loss at epo 530: 1.9105526579277856\n",
      "Loss at epo 540: 1.9051173554999488\n",
      "Loss at epo 550: 1.8998092804636275\n",
      "Loss at epo 560: 1.8946221636874334\n",
      "Loss at epo 570: 1.8895504734345845\n",
      "Loss at epo 580: 1.884589266351291\n",
      "Loss at epo 590: 1.879733539053372\n",
      "Loss at epo 600: 1.8749793197427478\n",
      "Loss at epo 610: 1.8703221844775335\n",
      "Loss at epo 620: 1.8657583615609579\n",
      "Loss at epo 630: 1.8612847634724208\n",
      "Loss at epo 640: 1.8568979838064739\n",
      "Loss at epo 650: 1.8525951623916626\n",
      "Loss at epo 660: 1.848373208301408\n",
      "Loss at epo 670: 1.8442299561841147\n",
      "Loss at epo 680: 1.840162953734398\n",
      "Loss at epo 690: 1.8361701445920127\n",
      "Loss at epo 700: 1.832249658022608\n",
      "Loss at epo 710: 1.8283992677927017\n",
      "Loss at epo 720: 1.8246175978864942\n",
      "Loss at epo 730: 1.8209027375493732\n",
      "Loss at epo 740: 1.8172533690929413\n",
      "Loss at epo 750: 1.8136681296995707\n",
      "Loss at epo 760: 1.8101457685232163\n",
      "Loss at epo 770: 1.8066848516464233\n",
      "Loss at epo 780: 1.8032843164035253\n",
      "Loss at epo 790: 1.7999427058867046\n",
      "Loss at epo 800: 1.7966589046376091\n",
      "Loss at epo 810: 1.7934318508420672\n",
      "Loss at epo 820: 1.7902605810335703\n",
      "Loss at epo 830: 1.7871436979089463\n",
      "Loss at epo 840: 1.7840800881385803\n",
      "Loss at epo 850: 1.781068674155644\n",
      "Loss at epo 860: 1.7781082425798689\n",
      "Loss at epo 870: 1.7751978320734842\n",
      "Loss at epo 880: 1.7723362637417657\n",
      "Loss at epo 890: 1.7695221249546325\n",
      "Loss at epo 900: 1.7667544279779708\n",
      "Loss at epo 910: 1.7640319432531084\n",
      "Loss at epo 920: 1.761353565113885\n",
      "Loss at epo 930: 1.7587180210011346\n",
      "Loss at epo 940: 1.7561240443161557\n",
      "Loss at epo 950: 1.7535705702645439\n",
      "Loss at epo 960: 1.751056359069688\n",
      "Loss at epo 970: 1.7485805609396525\n",
      "Loss at epo 980: 1.74614169384752\n",
      "Loss at epo 990: 1.7437387245041984\n",
      "Loss at epo 1000: 1.7413705042430332\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = 5\n",
    "W1 = Variable(torch.randn(embedding_dims, vocabulary_size).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(vocabulary_size, embedding_dims).float(), requires_grad=True)\n",
    "num_epochs = 1010\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x = Variable(get_input_layer(data)).float()\n",
    "        y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "        z1 = torch.matmul(W1, x)\n",
    "        z2 = torch.matmul(W2, z1)\n",
    "    \n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "\n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.data.item()\n",
    "        loss.backward()\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "    if epo % 10 == 0:    \n",
    "        print(f'Loss at epo {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "word2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
